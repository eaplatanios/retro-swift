// Copyright 2019, Emmanouil Antonios Platanios. All Rights Reserved.
//
// Licensed under the Apache License, Version 2.0 (the "License"); you may not
// use this file except in compliance with the License. You may obtain a copy of
// the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
// WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
// License for the specific language governing permissions and limitations under
// the License.

import TensorFlow

/// A driver takes steps in an environment using the provided policy.
public protocol Driver {
  associatedtype Agent: ReinforcementLearning.Agent

  typealias Observation = Agent.Observation
  typealias Action = Agent.Action
  typealias Reward = Agent.Reward
  typealias State = Agent.State

  typealias Listener = (Trajectory<Observation, Action, Reward, State>) -> Void

  /// Takes steps in the managed environment using the managed policy.
  @discardableResult
  mutating func run(
    startingIn state: State,
    using step: Step<Observation, Reward>,
    updating listeners: [Listener]
  ) -> Step<Observation, Reward>
}

public extension Driver where State == None {
  @discardableResult
  @inlinable
  mutating func run(
    using step: Step<Observation, Reward>,
    updating listeners: [Listener]
  ) -> Step<Observation, Reward> {
    run(startingIn: None(), using: step, updating: listeners)
  }
}

/// Trajectory generated by having an agent interact with an environment.
///
/// Trajectories consist of five main components, each of which can be a nested structure of
/// tensors with shapes whose first two dimensions are `[T, B]`, where `T` is the length of the
/// trajectory in terms of time steps and `B` is the batch size. The five components are:
///   - `stepKind`: Represents the kind of each time step (i.e., "first", "transition", or "last").
///     For example, if the agent takes an action in time step `t` that results in the current
///     episode ending, then `stepKind[t]` will be "last" and `stepKind[t + 1]` will be "first".
///   - `observation`: Observation that the agent receives from the environment in the beginning
///     of each time step.
///   - `action`: Action the agent took in each time step.
///   - `reward`: Reward that the agent received from the environment after each action. The reward
///     received after taking `action[t]` is `reward[t]`.
///   - `state`: The state of the agent after taking each action. The state the agent is in after
///     taking `action[t]` is `state[t]`.
public struct Trajectory<Observation, Action, Reward, State>: KeyPathIterable {
  // These need to be mutable because we use `KeyPathIterable.recursivelyAllWritableKeyPaths` to
  // automatically derive conformance to `Replayable`.
  public var stepKind: StepKind
  public var observation: Observation
  public var action: Action
  public var reward: Reward
  public var state: State

  public init(
    stepKind: StepKind,
    observation: Observation,
    action: Action,
    reward: Reward,
    state: State
  ) {
    self.stepKind = stepKind
    self.observation = observation
    self.action = action
    self.reward = reward
    self.state = state
  }
}

public struct StepBasedDriver<
  Environment: ReinforcementLearning.Environment,
  Agent: ReinforcementLearning.Agent
> where
  Environment.ObservationSpace.Value == Agent.Observation,
  Environment.ActionSpace.Value == Agent.Action,
  Environment.Reward == Agent.Reward,
  Agent.Action: Stackable,
  Agent.State: Stackable
{
  public let maxSteps: Int
  public let maxEpisodes: Int
  public let batchSize: Int

  public let batchedEnvironment: Bool
  public let batchedAgent: Bool

  public var environments: [Environment]
  public var agents: [Agent]

  public init(
    for environment: Environment,
    using agent: Agent,
    maxSteps: Int = Int.max,
    maxEpisodes: Int = Int.max,
    batchSize: Int = 1
  ) {
    precondition(maxSteps > 0 && maxEpisodes > 0, "'maxSteps' and 'maxEpisodes' must be > 0.")
    self.maxSteps = maxSteps
    self.maxEpisodes = maxEpisodes
    self.batchedEnvironment = environment.batched
    self.batchedAgent = agent.batched
    self.environments = batchedEnvironment ? 
      [environment] : 
      (0..<batchSize).map { _  in environment.copy() }
    self.agents = batchedAgent ? [agent] : [Agent](repeating: agent, count: batchSize)
    self.batchSize = batchSize
  }
}

extension StepBasedDriver: Driver {
  @discardableResult
  public mutating func run(
    startingIn state: State,
    using step: Step<Observation, Reward>,
    updating listeners: [Listener]
  ) -> Step<Observation, Reward> {
    if batchedAgent {
      agents[0].state = State.stack([State](repeating: state, count: batchSize))
    } else {
      agents.indices.forEach { agents[$0].state = state }
    }
    var currentStep = Step<Observation, Reward>.stack(
      [Step<Observation, Reward>](repeating: step, count: batchSize))
    var numSteps = 0
    var numEpisodes = 0
    while numSteps < maxSteps && numEpisodes < maxEpisodes {
      var action: Agent.Action
      var nextStep: Step<Observation, Reward>
      var state: Agent.State
      switch (batchedAgent, batchedEnvironment) {
      case (true, true):
        action = agents[0].action(for: currentStep)
        nextStep = environments[0].step(taking: action)
        state = agents[0].state
      case (true, false):
        action = agents[0].action(for: currentStep)
        let actions = action.unstacked()
        let nextSteps = environments.indices.map { environments[$0].step(taking: actions[$0]) }
        nextStep = Step<Observation, Reward>.stack(nextSteps)
        state = agents[0].state
      case (false, true):
        let currentSteps = currentStep.unstacked()
        let actions = agents.indices.map { agents[$0].action(for: currentSteps[$0]) }
        action = Agent.Action.stack(actions)
        nextStep = environments[0].step(taking: action)
        state = Agent.State.stack(agents.map { $0.state })
      case (false, false):
        let currentSteps = currentStep.unstacked()
        let actions = agents.indices.map { agents[$0].action(for: currentSteps[$0]) }
        action = Agent.Action.stack(actions)
        let nextSteps = environments.indices.map { environments[$0].step(taking: actions[$0]) }
        nextStep = Step<Observation, Reward>.stack(nextSteps)
        state = Agent.State.stack(agents.map { $0.state })
      }
      let trajectory = Trajectory(
        stepKind: nextStep.kind,
        observation: currentStep.observation,
        action: action,
        reward: nextStep.reward,
        state: state)
      listeners.forEach { $0(trajectory) }
      numSteps += Int((1 - Tensor<Int32>(trajectory.stepKind.isLast())).sum().scalar!)
      numEpisodes += Int(Tensor<Int32>(trajectory.stepKind.isLast()).sum().scalar!)
      currentStep = nextStep
    }
    return currentStep
  }
}

@discardableResult
public func runDriver<E: Environment, A: Agent>(
  environment: inout E,
  agent: inout A,
  maxSteps: Int = Int.max,
  maxEpisodes: Int = Int.max,
  batchSize: Int = 1,
  step: Step<A.Observation, A.Reward>,
  listeners: [(Trajectory<A.Observation, A.Action, A.Reward, A.State>) -> Void]
) -> Step<A.Observation, A.Reward> where
  E.ObservationSpace.Value == A.Observation,
  E.ActionSpace.Value == A.Action,
  E.Reward == A.Reward,
  A.Action: Stackable,
  A.State == None
{
  let state = None()
  let batchedEnvironment = environment.batched
  let batchedAgent = agent.batched
  var environments = batchedEnvironment ? 
    [environment] :
    (0..<batchSize).map { _  in environment.copy() }
  var agents = batchedAgent ? [agent] : [A](repeating: agent, count: batchSize)
  if batchedAgent {
    agents[0].state = A.State.stack([A.State](repeating: state, count: batchSize))
  } else {
    agents.indices.forEach { agents[$0].state = state }
  }
  var currentStep = step
  // var currentStep = Step<A.Observation, A.Reward>.stack(
  //   [Step<A.Observation, A.Reward>](repeating: step, count: batchSize))
  var numSteps = 0
  var numEpisodes = 0
  while numSteps < maxSteps && numEpisodes < maxEpisodes {
    var action: A.Action
    var nextStep: Step<A.Observation, A.Reward>
    var state: A.State
    switch (batchedAgent, batchedEnvironment) {
    case (true, true):
      action = agents[0].action(for: currentStep)
      nextStep = environments[0].step(taking: action)
      state = agents[0].state
    case (true, false):
      action = agents[0].action(for: currentStep)
      let actions = action.unstacked()
      let nextSteps = environments.indices.map { environments[$0].step(taking: actions[$0]) }
      nextStep = Step<A.Observation, A.Reward>.stack(nextSteps)
      state = agents[0].state
    case (false, true):
      let currentSteps = currentStep.unstacked()
      let actions = agents.indices.map { agents[$0].action(for: currentSteps[$0]) }
      action = A.Action.stack(actions)
      nextStep = environments[0].step(taking: action)
      state = A.State.stack(agents.map { $0.state })
    case (false, false):
      let currentSteps = currentStep.unstacked()
      let actions = agents.indices.map { agents[$0].action(for: currentSteps[$0]) }
      action = A.Action.stack(actions)
      let nextSteps = environments.indices.map { environments[$0].step(taking: actions[$0]) }
      nextStep = Step<A.Observation, A.Reward>.stack(nextSteps)
      state = A.State.stack(agents.map { $0.state })
    }
    let trajectory = Trajectory(
      stepKind: nextStep.kind,
      observation: currentStep.observation,
      action: action,
      reward: nextStep.reward,
      state: state)
    listeners.forEach { $0(trajectory) }
    numSteps += Int((1 - Tensor<Int32>(trajectory.stepKind.isLast())).sum().scalar!)
    numEpisodes += Int(Tensor<Int32>(trajectory.stepKind.isLast()).sum().scalar!)
    currentStep = nextStep
  }
  return currentStep
}
